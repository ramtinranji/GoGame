{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This program is written for Screening process of job application at goGame Company by Ramtin Ranji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I hereby declare that I am the sole writer of this code, and it's written based on the requirements of the screening project. This code has been tested on different machines to ensure its correctness "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The code is written by Anaconda, Python 2.7. In order to run the code, the following libraries are required: pandas, sqlite3, lorem and glob"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Project Description\n",
    "1) Generate N lines of input data (sample data attached) and splitting into different files\n",
    "2) Read generated input files from filesystem.\n",
    "3) Process Data.\n",
    "4) Generate reports for  :\n",
    "    a) Top 10 most frequently used words in all files.\n",
    "    b) Most frequently used word grouped by date.\n",
    "    c) Each User's total word counts.\n",
    "5) Write the results into a database.\n",
    "\n",
    "Constraints:\n",
    "Notes on generated data :\n",
    "    •\tN > 1000\n",
    "    •\tSplit file count = N/1000   (Eg if N is 1001 then two files one with 1000 lines and second one with 1 line)\n",
    "    •\tOne JSON string per line\n",
    "    •\tText size must be random with minimum 10 words maximum 50 words\n",
    "    •\tUser ID count = N / 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import requirements, for text generation I use lorem 0.1.1\n",
    "to install run \"pip install lorem\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#required for reading bunch of files from system\n",
    "import glob\n",
    "#required for generating a random text\n",
    "from lorem.text import TextLorem\n",
    "#required to read and write json objects\n",
    "import json\n",
    "#main python tools for Data Analysis\n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "#Required for SQL database operation\n",
    "import sqlite3\n",
    "#just some other libraries to work with!\n",
    "from datetime import datetime\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some requirements before actually start the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## normalize a list of words. All words must be same case and also\n",
    "## '.' character must be removed from a word to count the unique words\n",
    "## accurately. Otherwise 'Also' and 'also' might be considered as different \n",
    "## words. Same is true about '.Also' and 'Also'\n",
    "## The only punctuation in loren Ipsum is '.', for the actual text analysis\n",
    "## project other characters such as ',.!?' must be considered as well\n",
    "def normalize_text(text_list):\n",
    "    text_list = text_list.lower()\n",
    "    temp_text = text_list.replace('.','')\n",
    "    return temp_text\n",
    "\n",
    "# A function to generate a random date between 2018-1-1 to 2018-1-31\n",
    "def random_date():\n",
    "    day = random.randint(1,31)\n",
    "    sec = random.randint(0,59)\n",
    "    minute = random.randint(0,59)\n",
    "    hour = random.randint(0,23)\n",
    "    return datetime(2018,1,day,hour,minute,sec)\n",
    "\n",
    "#Remove previously generated text files, if the code runs more that once\n",
    "previous_text_files = glob.glob('ramtin*.txt')\n",
    "for f in previous_text_files:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Data generation step, change the number of N to your desired value, default set to 1001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Create files\n",
    "###############################\n",
    "###############################\n",
    "###############################\n",
    "\n",
    "#Initial Value for generating data    \n",
    "N = 1001\n",
    "\n",
    "#create N/3 number of users\n",
    "user_list = []\n",
    "for i in range(0,N/3):\n",
    "    temp = '0000000' + str(i)\n",
    "    user_list.append(temp[-4:])\n",
    "\n",
    "#A counter to control the filenames, each file must be seprated for each\n",
    "# 1000 lines of data\n",
    "file_count = 0\n",
    "\n",
    "#Generate N number of data\n",
    "for temp_counter in range(0,N):\n",
    "    # generate a random date between 2018-1-1 to 2018-1-31\n",
    "    date = random_date()\n",
    "    \n",
    "   # Text size must be random min 10 max 50, so the range of sentence is (5-10) words\n",
    "   # and the range of paragraph is (2-5) sentences, therefore in the minimum condition\n",
    "   # we have a paragraph 5*2 = 10 words and in the maximum condition we should have\n",
    "   # a 10*5 = 50 words \n",
    "    lorem = TextLorem(srange =(5,10),prange=(2,5))\n",
    "    # generate the random Text\n",
    "    text = lorem.paragraph()\n",
    "    mmlist = text.split()\n",
    "\n",
    "    #Build a JSON object, in Python, JSON equals to a dictionary\n",
    "    record = {\"text\":text,\n",
    "              \"user_id\":user_list[random.randint(0, len(user_list) -1)],\n",
    "              \"datetime\":str(date)\n",
    "              }\n",
    "    \n",
    "    #To split file for each 1000 lines of data we divide the current counter by 1000, \n",
    "    #and create a new file for each 1000 line of text\n",
    "    if(temp_counter % 1000 == 0):\n",
    "        file_count += 1\n",
    "        \n",
    "    #Generate the filename, a new filename for each 1000 lines of data\n",
    "    filename='ramtin'+str(file_count)+'.txt'\n",
    "    \n",
    "    #Append to the file\n",
    "    f = open(filename,'a')\n",
    "    \n",
    "    #Write the text as a JSON object to the file and close it\n",
    "    json.dump(record,f)\n",
    "    f.write('\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Reading from the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Read from files\n",
    "###############################\n",
    "###############################\n",
    "###############################\n",
    "\n",
    "#Our main dataframe to store all fetched data from filesystem\n",
    "dframe = DataFrame(columns=['text','user_id','datetime'])\n",
    "\n",
    "for sourceFile in (sorted(glob.glob('ramtin*.txt'))):\n",
    "    with open(sourceFile) as f:\n",
    "        for line in f:\n",
    "            #Read data as JSON object from file\n",
    "            data = json.loads(line)\n",
    "            #add JSON object to the main dataframe\n",
    "            dd = DataFrame(data,index =['0'])\n",
    "            dframe = pd.concat([dd,dframe],ignore_index = True,sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3: Processing Data\n",
    "###############################\n",
    "###############################\n",
    "###############################   \n",
    "            \n",
    "#Normalizing the text to remove '.' character and change all words to lower case\n",
    "dframe['text'] = map(lambda txt:normalize_text(txt),dframe['text'])\n",
    "\n",
    "#Add a new column to include the count of each text field in Data Frame\n",
    "dframe['count'] = map(lambda txt:len(txt),dframe['text'])\n",
    "\n",
    "#Fix the type of datetime field, Originally it's unicode, here it's changed\n",
    "#to datetime64\n",
    "dframe['datetime'] = map(lambda dd:pd.to_datetime(dd),dframe['datetime'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Data Analysis and producing reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4: Generate Reports\n",
    "###############################\n",
    "###############################\n",
    "###############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 most frequently used words in all files. Report A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Report A: Top 10 most frequently used words in all files\n",
    "\n",
    "#Copy all words into a list\n",
    "allwords = []\n",
    "for temp in dframe['text']:\n",
    "    allwords.extend(temp.split())\n",
    "\n",
    "#Convert the list to Series and count unique values with their frequency\n",
    "# in the series which contains all of the words from the files\n",
    "word_counts = Series(allwords).value_counts()\n",
    "\n",
    "#It contains 10 most frequently used words\n",
    "#Note: It might possible that the frequency of the 11th word be the same\n",
    "# as the 10th word, for example both repeated 100 times. Since you have\n",
    "# asked for 10 words, I use this method\n",
    "top10_words =  word_counts.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Showing Sample Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eius          1017\n",
      "voluptatem    1011\n",
      "adipisci      1002\n",
      "sed           1000\n",
      "numquam       1000\n",
      "ipsum          993\n",
      "etincidunt     992\n",
      "dolore         992\n",
      "porro          990\n",
      "non            986\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print top10_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most frequently used word grouped by date. Report B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Report B: Most frequently used word grouped by date.  \n",
    "    \n",
    "#Empty dataframe\n",
    "word_date_frame = DataFrame(columns=['Date','Frequency','Word'])\n",
    "\n",
    "#We use normalize to omit the effect of hours;minute;Second on the groupby\n",
    "for a in dframe.groupby(pd.DatetimeIndex(dframe['datetime']).normalize()):\n",
    "    \n",
    "    #Store all the words in each day in a list\n",
    "    all_words_date = []\n",
    "    for tt in a[1]['text']:\n",
    "        all_words_date.extend(tt.split())\n",
    "    \n",
    "    #Count Unique words with their frequency in each day\n",
    "    word_series = Series(all_words_date).value_counts()\n",
    "    \n",
    "    \n",
    "    for x in range (word_series.count()):\n",
    "        #Check to see if the current words is repeated most\n",
    "        #It might possible that more than one word reated most\n",
    "        #so it must be checked first\n",
    "        if word_series[x] == word_series.max():\n",
    "            #Add a record to the dataframe\n",
    "            record = {\"Date\":[a[0]],\n",
    "              \"Frequency\":[word_series[x]],\n",
    "              \"Word\":[word_series.index.values[x]] \n",
    "              }\n",
    "            temp_frame = DataFrame(record)\n",
    "            word_date_frame = pd.concat([temp_frame,word_date_frame],ignore_index = True,sort=False)\n",
    "#Sort values of dataframe by Date\n",
    "word_date_frame = word_date_frame.sort_values(by='Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Showing Sample results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date Frequency        Word\n",
      "33 2018-01-01        42         non\n",
      "32 2018-01-02        52       velit\n",
      "31 2018-01-03        33  etincidunt\n",
      "30 2018-01-04        39    quisquam\n",
      "29 2018-01-04        39      labore\n",
      "28 2018-01-04        39      magnam\n",
      "27 2018-01-05        51  voluptatem\n",
      "26 2018-01-06        34       dolor\n",
      "25 2018-01-07        51       neque\n",
      "24 2018-01-08        35     dolorem\n",
      "23 2018-01-09        35      dolore\n",
      "22 2018-01-10        37        eius\n",
      "21 2018-01-11        51    adipisci\n",
      "20 2018-01-12        44      labore\n",
      "19 2018-01-13        34  voluptatem\n"
     ]
    }
   ],
   "source": [
    "print word_date_frame.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Each User's total word counts. Report C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report C: Each User's total word counts\n",
    "#Since in data processing step, I've created \"Count\" column\n",
    "# it can be calculated easily with a groupby on the users and then sum it\n",
    "user_sum = dframe.groupby(['user_id']).sum()\n",
    "#create a dataframe to easily pass into the SQL\n",
    "user_result = DataFrame({'User_id': user_sum.index,'TotalWord':user_sum['count']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show sample results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id\n",
      "0001    680\n",
      "0002    121\n",
      "0003    472\n",
      "0004    698\n",
      "0005    267\n",
      "Name: TotalWord, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print user_result['TotalWord'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Write the results into a database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5: Write the results into a database\n",
    "\n",
    "#Create a Database if it's not already exist or connect to it if it's existed\n",
    "sqlconnection = sqlite3.connect('goGame-Ramtin.db')\n",
    "\n",
    "\n",
    "sql_create_TopTen_table = \"\"\" CREATE TABLE IF NOT EXISTS TopTen (\n",
    "                                        id integer PRIMARY KEY,\n",
    "                                        Word text NOT NULL,\n",
    "                                        Frequency integer NOT NULL,\n",
    "                                        AnalysisDate datetime NOT NULL\n",
    "                                    ); \"\"\"\n",
    "\n",
    "sql_create_MostWords_byDate_table = \"\"\" CREATE TABLE IF NOT EXISTS MostWords_byDate (\n",
    "                                        id integer PRIMARY KEY,\n",
    "                                        Word text NOT NULL,\n",
    "                                        Date datetime NOT NULL,\n",
    "                                        Frequency integer NOT NULL,\n",
    "                                        AnalysisDate datetime NOT NULL\n",
    "                                    ); \"\"\"\n",
    "sql_create_User_Total_Words_table = \"\"\" CREATE TABLE IF NOT EXISTS User_Total_Words (\n",
    "                                        id integer PRIMARY KEY,\n",
    "                                        User_id text NOT NULL,\n",
    "                                        AnalysisDate datetime NOT NULL,\n",
    "                                        TotalWord integer NOT NULL                                        \n",
    "                                    ); \"\"\"\n",
    "curser = sqlconnection.cursor()\n",
    "\n",
    "curser.execute(sql_create_TopTen_table)\n",
    "curser.execute(sql_create_MostWords_byDate_table)\n",
    "curser.execute(sql_create_User_Total_Words_table)\n",
    "\n",
    "# record the time of analysis\n",
    "now = datetime.now()\n",
    "\n",
    "#Write the results of the Report A:Top 10 most frequently used words in all files\n",
    "#into the database\n",
    "report2_frame = DataFrame({'Word':top10_words.index,'Frequency':top10_words.values,'AnalysisDate':now})\n",
    "report2_frame.to_sql('TopTen',sqlconnection,index=False,if_exists='append')\n",
    "\n",
    "#Write the results of the Report B:Most frequently used word grouped by date\n",
    "#into the database\n",
    "word_date_frame['AnalysisDate'] = now\n",
    "word_date_frame.to_sql('MostWords_byDate',sqlconnection,index=False,if_exists='append')\n",
    "\n",
    "#Write the results of the report C: Each User's total word counts\n",
    "#into the database\n",
    "user_result['AnalysisDate'] = now\n",
    "user_result.to_sql('User_Total_Words',sqlconnection,index=False,if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read from SQL Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id        Word  Frequency                AnalysisDate\n",
      "0    1     tempora       1014  2019-01-12 23:48:11.380000\n",
      "1    2     quaerat        989  2019-01-12 23:48:11.380000\n",
      "2    3       porro        987  2019-01-12 23:48:11.380000\n",
      "3    4     dolorem        987  2019-01-12 23:48:11.380000\n",
      "4    5        modi        981  2019-01-12 23:48:11.380000\n",
      "5    6  etincidunt        979  2019-01-12 23:48:11.380000\n",
      "6    7    adipisci        978  2019-01-12 23:48:11.380000\n",
      "7    8       neque        977  2019-01-12 23:48:11.380000\n",
      "8    9      dolore        973  2019-01-12 23:48:11.380000\n",
      "9   10         sed        973  2019-01-12 23:48:11.380000\n",
      "10  11     tempora       1014  2019-01-12 23:56:49.374000\n",
      "11  12     quaerat        989  2019-01-12 23:56:49.374000\n",
      "12  13       porro        987  2019-01-12 23:56:49.374000\n",
      "13  14     dolorem        987  2019-01-12 23:56:49.374000\n",
      "14  15        modi        981  2019-01-12 23:56:49.374000\n"
     ]
    }
   ],
   "source": [
    "#SQL query to read data from a table\n",
    "sql_read_TopTen_query = \"\"\" SELECT * FROM TopTen \"\"\"\n",
    "\n",
    "#Read table data into a dataframe\n",
    "df = pd.read_sql(sql_read_TopTen_query,sqlconnection)\n",
    "\n",
    "#show the first 15 rows of dataframe\n",
    "print df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***** Please contact me if you need any clarification about the code\n",
    "***** ramtinranji@gmail.com\n",
    "***** ramtin_ranji@yahoo.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
